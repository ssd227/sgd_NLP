{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_nlp/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sgd_nlp.transformer import MultiheadAttention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块1: MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 4\n",
    "N = 10\n",
    "T, d = 100, 64\n",
    "M = torch.triu(-float(\"inf\")*torch.ones(T,T),1)\n",
    "X = torch.randn(N,T,d)\n",
    "\n",
    "# pytorch 实现\n",
    "attn_ = nn.MultiheadAttention(d, heads, bias=False, batch_first=True)\n",
    "Y_, A_ = attn_(X,X,X, attn_mask=M)\n",
    "\n",
    "# sgd_nlp库实现\n",
    "attn = MultiheadAttention(d, heads, bias=False)\n",
    "# init param\n",
    "attn.proj_k.weight.data, attn.proj_q.weight.data, attn.proj_v.weight.data  = torch.split(attn_.in_proj_weight.detach(), split_size_or_sections=d, dim=0)\n",
    "attn.out_proj.weight.data = attn_.out_proj.weight.detach()\n",
    "\n",
    "Y, A = attn(X,X,X,attn_mask=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape, Y_.shape)\n",
    "print(A.shape, A_.shape)\n",
    "print(torch.dist(Y.detach(), Y_.detach()))\n",
    "print(torch.dist(A.mean(1).detach(), A_.detach()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块2: TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.transformer import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 4\n",
    "num_layers = 2\n",
    "\n",
    "N, T, d = 10, 100, 64\n",
    "M = torch.triu(-float(\"inf\")*torch.ones(T,T),1)\n",
    "X = torch.randn(N,T,d)\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=d, nhead=heads, dim_feedforward=128)\n",
    "out_layer = encoder_layer(X)\n",
    "\n",
    "print(\"encoder layer output\", out_layer.shape)\n",
    "\n",
    "encoder = TransformerEncoder(num_layers, d_model=d, nhead=heads, dim_feedforward=128)\n",
    "out_encoder = encoder(X)\n",
    "\n",
    "print(\"encoder stack output\", out_encoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块3: TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.transformer import TransformerDecoder, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = torch.triu(-float(\"inf\")*torch.ones(T, T), 1)\n",
    "\n",
    "decoder_layer = TransformerDecoderLayer(d_model=d, nhead=heads, dim_feedforward=128)\n",
    "out_layer = decoder_layer(X, out_encoder, tgt_mask)\n",
    "\n",
    "print(\"decoder layer output\", out_layer.shape)\n",
    "\n",
    "decoder = TransformerDecoder(num_layers, d_model=d, nhead=heads, dim_feedforward=128)\n",
    "out_decoder = decoder(X, out_encoder, tgt_mask)\n",
    "\n",
    "print(\"decoder stack output\", out_decoder.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块4: 位置编码\n",
    "模拟二进制\n",
    "\n",
    "位置  0    1    2     3     4     5    6   7\n",
    "\n",
    "编码  000  001  010   011   100   101  110 111\n",
    "\n",
    "\n",
    "行代表词元在序列中的位置，列代表位置编码的不同维度\n",
    "每个列维度使用不同的频率，变换频率依次降低。 类似于二进制的高阶数字变化较慢（100）\n",
    "词元的行位置交替使用sin cos来生存每一个列维度对应的具体数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sgd_nlp.transformer import PositionalEncoding\n",
    "\n",
    "batchs, num_steps, encoding_dim = 1, 200, 31\n",
    "pos_encoding = PositionalEncoding(num_hiddens=encoding_dim, max_len=num_steps)\n",
    "pos_encoding.eval()\n",
    "\n",
    "X = torch.zeros((batchs, num_steps, encoding_dim))\n",
    "xe = pos_encoding(X)\n",
    "print(xe.shape)\n",
    "\n",
    "xx = torch.arange(num_steps)\n",
    "yy1 = xe[0, :, 4].reshape(-1)\n",
    "yy2 = xe[0, :, 10].reshape(-1)\n",
    "yy3 = xe[0, :, 12].reshape(-1)\n",
    "yy4 = xe[0, :, 20].reshape(-1)\n",
    "\n",
    "plt.plot(xx, yy1)\n",
    "plt.plot(xx, yy2)\n",
    "plt.plot(xx, yy3)\n",
    "plt.plot(xx, yy4)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块整合 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.transformer import Transformer\n",
    "\n",
    "heads = 4\n",
    "N, T, d = 10, 100, 64\n",
    "T_ = 60\n",
    "\n",
    "M = torch.triu(-float(\"inf\")*torch.ones(T,T),1)\n",
    "X = torch.randn(N,T,d)\n",
    "\n",
    "src = torch.randn(N,T,d)\n",
    "tgt = torch.randn(N,T_,d)\n",
    "\n",
    "transformer = Transformer(d_model=d, nhead=heads, num_encoder_layers= 6,\n",
    "                num_decoder_layers=4, dim_feedforward = d*4, dropout=0.1)\n",
    "out = transformer(src, tgt)\n",
    "print(\"transformer out\", out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练语言模型\n",
    "\n",
    "使用transformer encoder训练语言模型（类似 ./apps/RNN/word_level_language_model.ipynb）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "\n",
    "# github raw-file下载有问题，手动下载对应文件到data目录\n",
    "# ptb_data = \"https://github.com/wojzaremba/lstm/blob/master/data/ptb.\"\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        print(ptb_data + f)\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import sgd_nlp\n",
    "import numpy as np\n",
    "\n",
    "from sgd_nlp.simple_training import train_ptb, evaluate_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.transformer import PositionalEncoding\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, output_size,  num_encoder_layers= 6,\n",
    "                num_decoder_layers=4, dim_feedforward = 1024, dropout=0.1,\n",
    "                device=None, dtype=torch.float32):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.emb = nn.Embedding(num_embeddings=output_size, embedding_dim=d_model, **factory_kwargs)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.seq_model = TransformerEncoder(num_layers=num_encoder_layers, d_model=d_model, nhead=n_heads,\n",
    "                dim_feedforward = dim_feedforward, dropout=dropout, **factory_kwargs)\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, output_size, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        \"\"\"\n",
    "        Given sequence (and the previous hidden state if given), returns probabilities of next word\n",
    "        (along with the last hidden state from the sequence model).\n",
    "        Inputs:\n",
    "        x of shape (seq_len, bs)\n",
    "        h of shape (num_layers, bs, hidden_size) if using RNN,\n",
    "            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)\n",
    "        Returns (out, h)\n",
    "        out of shape (seq_len*bs, output_size)\n",
    "        h of shape (num_layers, bs, hidden_size) if using RNN,\n",
    "            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)\n",
    "        \"\"\"\n",
    "        T, B = x.shape\n",
    "        # 缺少了位置编码\n",
    "        # 训练集x的次序也给搞错了\n",
    "        x = x.swapaxes(0, 1) # [B,T]\n",
    "        src = self.pos_emb(self.emb(x)) #[B,T,C]\n",
    "        out = self.seq_model(src) #[B,T,C]\n",
    "        out = out.swapaxes(0, 1) #[T,B,C]\n",
    "        \n",
    "        y =  self.linear(out.reshape((T*B, self.d_model))) #[T*B, C_out]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方的实现\n",
    "\n",
    "from sgd_nlp.transformer import PositionalEncoding\n",
    "\n",
    "class LanguageModel_Official(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, output_size,  num_encoder_layers= 6,\n",
    "                num_decoder_layers=4, dim_feedforward = 1024, dropout=0.1,\n",
    "                device=None, dtype=torch.float32):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(LanguageModel_Official, self).__init__()\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.emb = nn.Embedding(num_embeddings=output_size, embedding_dim=d_model, **factory_kwargs)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        layer_factory_kwargs = {'d_model':d_model, 'nhead':n_heads, 'dim_feedforward':dim_feedforward,\n",
    "                    'dropout':dropout, 'device': device, 'dtype': dtype}\n",
    "        # encoder_layers = nn.TransformerEncoderLayer(batch_first=True, **layer_factory_kwargs)\n",
    "        # self.seq_model = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        # 手动堆层\n",
    "        self.seq_model = nn.Sequential(*[nn.TransformerEncoderLayer(batch_first=True, **layer_factory_kwargs) for _ in range(num_encoder_layers)])\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, output_size, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        \"\"\"\n",
    "        Given sequence (and the previous hidden state if given), returns probabilities of next word\n",
    "        (along with the last hidden state from the sequence model).\n",
    "        Inputs:\n",
    "        x of shape (seq_len, bs)\n",
    "        h of shape (num_layers, bs, hidden_size) if using RNN,\n",
    "            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)\n",
    "        Returns (out, h)\n",
    "        out of shape (seq_len*bs, output_size)\n",
    "        h of shape (num_layers, bs, hidden_size) if using RNN,\n",
    "            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)\n",
    "        \"\"\"\n",
    "        T, B = x.shape\n",
    "        # 缺少了位置编码\n",
    "        # 训练集x的次序也给搞错了\n",
    "        x = x.swapaxes(0, 1) # [B,T]\n",
    "        src = self.pos_emb(self.emb(x)) #[B,T,C]\n",
    "        out = self.seq_model(src) #[B,T,C]\n",
    "        out = out.swapaxes(0, 1) #[T,B,C]\n",
    "        \n",
    "        y =  self.linear(out.reshape((T*B, self.d_model))) #[T*B, C_out]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义具体训练过程\n",
    "\n",
    "from sgd_nlp.data import get_batch\n",
    "from sgd_nlp.simple_training import accuracy\n",
    "\n",
    "### PTB training ###\n",
    "def epoch_transformer(data, model, seq_len, loss_fn=nn.CrossEntropyLoss(), opt=None,\n",
    "        clip=None, device=None, dtype=None):\n",
    "    \"\"\"\n",
    "    Iterates over the data. If optimizer is not None, sets the\n",
    "    model to train mode, and for each batch updates the model parameters.\n",
    "    If optimizer is None, sets the model to eval mode, and simply computes\n",
    "    the loss/accuracy.\n",
    "\n",
    "    Args:\n",
    "        data: data of shape (nbatch, batch_size) given from batchify function\n",
    "        model: LanguageModel instance\n",
    "        seq_len: i.e. bptt, sequence length\n",
    "        loss_fn: nn.Module instance\n",
    "        opt: Optimizer instance (optional)\n",
    "        clip: max norm of gradients (optional)\n",
    "\n",
    "    Returns:\n",
    "        avg_acc: average accuracy over dataset\n",
    "        avg_loss: average loss over dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(4)\n",
    "    if opt == None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    nbatch, batch_size = data.shape\n",
    "    accum_loss = 0\n",
    "    accum_acc = 0\n",
    "    sum_samples = 0\n",
    "    \n",
    "    for i in range(0, nbatch - 1, seq_len):\n",
    "        batch_x, batch_y = get_batch(data, i, seq_len, device=device, dtype=dtype)\n",
    "        sum_samples += batch_y.shape[0]\n",
    "        \n",
    "        if opt == None:\n",
    "            out = model(batch_x)\n",
    "            loss = loss_fn(out, batch_y)\n",
    "        else:\n",
    "            opt.zero_grad()\n",
    "            out = model(batch_x)\n",
    "            loss = loss_fn(out, batch_y)\n",
    "            loss.backward()\n",
    "            if getattr(opt, 'clip_grad_norm', None) is not None:\n",
    "                if clip is not None:\n",
    "                    opt.clip_grad_norm(clip)\n",
    "                else:\n",
    "                    opt.clip_grad_norm()\n",
    "            opt.step()\n",
    "        \n",
    "        cur_batch_loss = loss.detach()\n",
    "        cur_batch_succ = accuracy(out, batch_y)\n",
    "        accum_loss +=  cur_batch_loss\n",
    "        accum_acc += cur_batch_succ\n",
    "        # if i%100==0:\n",
    "        #     print(\"done:[{}], left:[{}], total:[{}]\".format(i, nbatch-i, nbatch))\n",
    "        #     print(\"batch:{} \\t batch_loss:[{}] \\t batch_acc:[{}]\".format(i, cur_batch_loss, cur_batch_succ))\n",
    "        #     print()\n",
    "    return accum_acc*(1.0/sum_samples), accum_loss * (1.0/sum_samples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练超参数\n",
    "# device = torch.device('cpu')   \n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "n_heads = 8\n",
    "num_layers=2 # Transformer层数\n",
    "n_epochs=100 # 数据遍历次数\n",
    "\n",
    "# embedding_size=400 # word编码维度\n",
    "d_model = 1024  # hidden dim\n",
    "\n",
    "seq_len = 21 # truncated BPTT 序列截断长度\n",
    "batch_size = 256 # 批处理数量\n",
    "\n",
    "optimizer=torch.optim.Adam\n",
    "lr=1e-4 # 学习率\n",
    "weight_decay=0\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 加载训练数据\n",
    "corpus = sgd_nlp.data.Corpus(\"data/ptb\", max_lines=2000)\n",
    "train_data = sgd_nlp.data.batchify(corpus.train, batch_size=batch_size, device=device, dtype=np.float32)\n",
    "print(train_data.shape)\n",
    "\n",
    "\n",
    "model = LanguageModel(d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    output_size=len(corpus.dictionary),\n",
    "                    num_encoder_layers= num_layers,\n",
    "                    num_decoder_layers= num_layers,\n",
    "                    dim_feedforward = d_model*2,\n",
    "                    dropout=0.1,\n",
    "                    device=device,\n",
    "                    dtype=torch.float32)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n",
    "# model = LanguageModel_Official(d_model=d_model,\n",
    "#                     n_heads=n_heads,\n",
    "#                     output_size=len(corpus.dictionary),\n",
    "#                     num_encoder_layers= num_layers,\n",
    "#                     num_decoder_layers= num_layers,\n",
    "#                     dim_feedforward = d_model*2,\n",
    "#                     dropout=0.1,\n",
    "#                     device=device,\n",
    "#                     dtype=torch.float32)\n",
    "\n",
    "print('---------------')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n",
    "# print(next(model.seq_model.encoder_layers[0].parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptb(model, \n",
    "          train_data, \n",
    "          seq_len=seq_len, \n",
    "          n_epochs=n_epochs, \n",
    "          device=device, \n",
    "          optimizer=optimizer, \n",
    "          lr=lr, \n",
    "          weight_decay=weight_decay, \n",
    "          loss_fn=loss_fn,\n",
    "          epoch_func=epoch_transformer,)\n",
    "\n",
    "evaluate_ptb(model,\n",
    "             train_data,\n",
    "             seq_len=seq_len,\n",
    "             epoch_func=epoch_transformer,\n",
    "             device=device,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验小结：\n",
    "\n",
    "使用时注意 BTC的顺序是否满足\n",
    "加上positional encoding，模型收敛速度要快得多\n",
    "\n",
    "整体的计算效率确实要比RNN快，但是区别在于 Td^2 和T^2d的区别\n",
    "如何涉及非常大的时间序列依赖， T远超d。 d一般取2014，2048，再大就很难算了\n",
    "\n",
    "当初写Transformer、bert的人想不到后续能有这么大的影响力。\n",
    "adam 1e-4, 太大loss降不下去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[1,2,3,4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的transformer seq2seq训练过程，参考./apps/Transformer/pos_tagging.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
