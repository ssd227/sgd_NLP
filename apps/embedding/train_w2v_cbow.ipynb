{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec (Cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_nlp\n"
     ]
    }
   ],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_nlp/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.embedding import Cbow, CorpusFactoryCbow\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取语料数据\n",
    "def load_corpus(corpus_dir_path, load_obj=False, obj_file_name=None):\n",
    "    \n",
    "    if load_obj and os.path.isfile(obj_file_name):\n",
    "        with open(obj_file_name, 'rb') as fin:\n",
    "            print(\"!!! load corpus factory success !!!\")\n",
    "            return pickle.load(fin)\n",
    "    else:\n",
    "        print('CURRENT PATH:\\t', corpus_dir_path)\n",
    "\n",
    "        corpus_factory = CorpusFactoryCbow(corpus_dir_path)  # new obj from origin corpus file path\n",
    "        corpus_factory.vocab.log_info()\n",
    "        with open(obj_file_name, 'wb') as fout:\n",
    "            pickle.dump(corpus_factory, fout)\n",
    "        return corpus_factory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus_factory, model, optimizer, scheduler, config):\n",
    "    all_words_num = corpus_factory.vocab.corpus_word_count  # 文档中的总词数\n",
    "    epoch = int(config.corpus_run_loop * all_words_num / config.batch_size)  # 总共需要迭代几个epoch\n",
    "    global_min_loss = 1e6\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epoch):\n",
    "        t1 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        batch_data = corpus_factory.training_batch(batch_num=config.batch_size,\n",
    "                                                   device=config.device,\n",
    "                                                   win_width=config.win_width,\n",
    "                                                   neg_k=config.neg_k)\n",
    "        y = model.forward(batch_data)\n",
    "\n",
    "        # objective function (loss function)\n",
    "        j_theta = torch.sum(y, dim=[1, 2]).mean()  # maximize objective\n",
    "        nj_theta = -1 * j_theta  # minimize objective\n",
    "\n",
    "        # backward and update weight\n",
    "        nj_theta.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % config.scheduler_step == 0:\n",
    "            scheduler.step()\n",
    "\n",
    "        # output info\n",
    "        tmp_t = time.time() - t1\n",
    "        # avg_time = avg_time * 0.9 + 0.1 * tmp_t\n",
    "        if i % config.log_step == 0:\n",
    "            print('epoch:{}/{}, loss:{}, cost_time: {}'.format(i, epoch, nj_theta, tmp_t))\n",
    "\n",
    "        # save best model\n",
    "        if nj_theta < global_min_loss:\n",
    "            global_min_loss = nj_theta\n",
    "            torch.save(model.state_dict(), config.model_weights_obj_path)\n",
    "            print('new bset loss: {}'.format(nj_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    # 文本语料路径\n",
    "    data_home = r'./data'\n",
    "    sub_dir = r'friends/season10'\n",
    "    corpus_dir_path = os.path.join(data_home, sub_dir)\n",
    "    \n",
    "    # 直接加载对象\n",
    "    SAVE_HOME = r'./apps/embedding/save/'\n",
    "    model_name = r'cbow'\n",
    "    \n",
    "    load_corpus_obj = True # 训练前修改！\n",
    "    corpus_obj_path = os.path.join(SAVE_HOME, model_name, r'corpus_obj.cf') # 加载预处理语料  default:None\n",
    "     \n",
    "    load_model_weight_obj = True # 训练前修改！\n",
    "    model_weights_obj_path = os.path.join(SAVE_HOME, model_name, r'cbow_weights.path') # 加载预训练模型参数 default:None\n",
    "    \n",
    "    # 语料预处理参数\n",
    "    win_width = 11  # context 窗口大小（前5-中间词-后5）\n",
    "    neg_k = 10  # 负采样数\n",
    "    \n",
    "    # 模型参数\n",
    "    device = torch.device('cuda')\n",
    "    emb_dim = 300\n",
    "    \n",
    "    # 训练参数\n",
    "    lr = 1e-2 # 初始学习率\n",
    "    corpus_run_loop = 2  # 看n遍文本\n",
    "    batch_size = 1024   # 每个batch的大小\n",
    "    \n",
    "    scheduler_step = 20\n",
    "    log_step = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def app():\n",
    "    # class obj\n",
    "    corpus_factory = load_corpus(corpus_dir_path=config.corpus_dir_path,\n",
    "                                 load_obj=config.load_corpus_obj,\n",
    "                                 obj_file_name=config.corpus_obj_path)\n",
    "    \n",
    "    model = Cbow(emb_dim=config.emb_dim,\n",
    "                 token_num=corpus_factory.token_num(),\n",
    "                 win_width=config.win_width,\n",
    "                 sparse_emb=True).to(config.device)\n",
    "\n",
    "    # load weight\n",
    "    if config.load_model_weight_obj and os.path.isfile(config.model_weights_obj_path):\n",
    "        model.load_state_dict(torch.load(config.model_weights_obj_path))\n",
    "        print(\"!!! Load model weights success !!!\")\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SparseAdam(params=model.parameters(), lr=config.lr)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    train(corpus_factory=corpus_factory,\n",
    "          model=model,\n",
    "          optimizer=optimizer,\n",
    "          scheduler=scheduler,\n",
    "          config=config,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! load corpus factory success !!!\n",
      "!!! Load model weights success !!!\n",
      "epoch:0/1732, loss:2.555410623550415, cost_time: 0.03940176963806152\n",
      "new bset loss: 2.555410623550415\n",
      "new bset loss: 1.6718511581420898\n",
      "new bset loss: 1.6290907859802246\n",
      "new bset loss: 1.6108152866363525\n",
      "new bset loss: 1.5397448539733887\n",
      "epoch:50/1732, loss:2.9481301307678223, cost_time: 0.03519773483276367\n",
      "new bset loss: 1.5265324115753174\n",
      "new bset loss: 1.30142343044281\n",
      "new bset loss: 1.1666502952575684\n",
      "new bset loss: 1.0776259899139404\n",
      "epoch:100/1732, loss:2.0683443546295166, cost_time: 0.03653311729431152\n",
      "epoch:150/1732, loss:1.9276400804519653, cost_time: 0.0360560417175293\n",
      "epoch:200/1732, loss:1.3475074768066406, cost_time: 0.03653550148010254\n",
      "epoch:250/1732, loss:2.508596897125244, cost_time: 0.034174203872680664\n",
      "epoch:300/1732, loss:2.3538296222686768, cost_time: 0.03573465347290039\n",
      "epoch:350/1732, loss:2.806239366531372, cost_time: 0.03628849983215332\n",
      "epoch:400/1732, loss:5.390937805175781, cost_time: 0.035471439361572266\n",
      "epoch:450/1732, loss:1.415923833847046, cost_time: 0.03495359420776367\n",
      "new bset loss: 1.0349793434143066\n",
      "epoch:500/1732, loss:3.7410926818847656, cost_time: 0.03773188591003418\n",
      "epoch:550/1732, loss:2.002063274383545, cost_time: 0.03504824638366699\n",
      "epoch:600/1732, loss:2.088330030441284, cost_time: 0.037531137466430664\n",
      "epoch:650/1732, loss:4.992619514465332, cost_time: 0.036432504653930664\n",
      "epoch:700/1732, loss:1.6476190090179443, cost_time: 0.03729510307312012\n",
      "epoch:750/1732, loss:1.5113050937652588, cost_time: 0.03770089149475098\n",
      "epoch:800/1732, loss:1.6178252696990967, cost_time: 0.03524589538574219\n",
      "new bset loss: 0.9378471970558167\n",
      "epoch:850/1732, loss:2.3311376571655273, cost_time: 0.04049277305603027\n",
      "epoch:900/1732, loss:1.1542460918426514, cost_time: 0.04027891159057617\n",
      "epoch:950/1732, loss:2.5259995460510254, cost_time: 0.03500556945800781\n",
      "new bset loss: 0.7275902032852173\n",
      "epoch:1000/1732, loss:4.2186665534973145, cost_time: 0.03813314437866211\n",
      "epoch:1050/1732, loss:3.1831769943237305, cost_time: 0.038819313049316406\n",
      "epoch:1100/1732, loss:1.5821566581726074, cost_time: 0.03587675094604492\n",
      "new bset loss: 0.6533079743385315\n",
      "epoch:1150/1732, loss:2.811288833618164, cost_time: 0.038574934005737305\n",
      "epoch:1200/1732, loss:1.506406307220459, cost_time: 0.04114961624145508\n",
      "new bset loss: 0.6148627996444702\n",
      "new bset loss: 0.5540788769721985\n",
      "epoch:1250/1732, loss:1.1733076572418213, cost_time: 0.040065765380859375\n",
      "epoch:1300/1732, loss:1.946364164352417, cost_time: 0.0413362979888916\n",
      "epoch:1350/1732, loss:2.757241725921631, cost_time: 0.040643930435180664\n",
      "epoch:1400/1732, loss:1.6469314098358154, cost_time: 0.037580251693725586\n",
      "epoch:1450/1732, loss:1.6014111042022705, cost_time: 0.03676629066467285\n",
      "epoch:1500/1732, loss:1.4459469318389893, cost_time: 0.04819512367248535\n",
      "epoch:1550/1732, loss:2.077948808670044, cost_time: 0.04598116874694824\n",
      "epoch:1600/1732, loss:2.201108932495117, cost_time: 0.040940284729003906\n",
      "epoch:1650/1732, loss:1.765796422958374, cost_time: 0.03723502159118652\n",
      "epoch:1700/1732, loss:2.033046007156372, cost_time: 0.03829312324523926\n"
     ]
    }
   ],
   "source": [
    "app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
