{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a word-level language model\n",
    "\n",
    "#### 实验结论：\n",
    "* 序列建模，对下一位的预测效果还算符合预期。\n",
    "* 训练时间至少 200-500个epoch\n",
    "* adam lr=1e-3\n",
    "* 目前最多才测试了1000行语料，提高语料规模后训练速度很慢。\n",
    "\n",
    "#### 处理不同序列长度的一些策略 [todo]\n",
    "* Padding（input补上0，直接计算，结果再针对性截断，比较简单）\n",
    "\t但是对于长度特别不一致的序列，会浪费很多计算资源\n",
    "* Packed sequence 打包序列法，每个时间步骤叠加，记录初始结束位置。\n",
    "  \n",
    "#### 训练lstm (本文仅使用truncated BPTT进行训练)\n",
    "* truncated BPTT and hidden repackaging（缺点，长依赖丢失）\n",
    "* 记录下最后的h和c，使用detach把上一轮的计算图消掉。（有了长依赖，介于BPTT 和 truncated BPTT之间）[todo]\n",
    "\n",
    "#### TODO:\n",
    "* 比较pytorch官方实现的效率和效果 \n",
    "* grad clip功能\n",
    "* 完整的训练过程，使用 train、valid、test数据选择best model parameters\n",
    "* 训练过程图形可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_nlp/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "\n",
    "# github raw-file下载有问题，手动下载对应文件到data目录\n",
    "# ptb_data = \"https://github.com/wojzaremba/lstm/blob/master/data/ptb.\"\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        print(ptb_data + f)\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sgd_nlp\n",
    "import numpy as np\n",
    "from sgd_nlp.models import LanguageModel\n",
    "from sgd_nlp.simple_training import train_ptb, evaluate_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练超参数\n",
    "# device = torch.device('cpu')   \n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "num_layers=2 # RNN层数\n",
    "n_epochs=200 # 数据遍历次数\n",
    "\n",
    "embedding_size=400 # word编码维度\n",
    "hidden_size = 1150  # hidden dim\n",
    "\n",
    "seq_len = 20 # truncated BPTT 序列截断长度\n",
    "batch_size = 200 # 批处理数量\n",
    "\n",
    "optimizer=torch.optim.Adam\n",
    "lr=1e-4 # 学习率\n",
    "weight_decay=0\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 加载训练数据\n",
    "corpus = sgd_nlp.data.Corpus(\"data/ptb\", max_lines=1000)\n",
    "train_data = sgd_nlp.data.batchify(corpus.train, batch_size=batch_size, device=device, dtype=np.float32)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确认模型参数正确\n",
    "model = LanguageModel(embedding_size=embedding_size,\n",
    "                      output_size=len(corpus.dictionary),\n",
    "                      hidden_size=hidden_size,\n",
    "                      num_layers=num_layers,\n",
    "                      seq_model='rnn',\n",
    "                      device=device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LanguageModel(embedding_size=embedding_size,\n",
    "                      output_size=len(corpus.dictionary),\n",
    "                      hidden_size=hidden_size,\n",
    "                      num_layers=num_layers,\n",
    "                      seq_model='rnn',\n",
    "                      device=device)\n",
    "\n",
    "train_ptb(model, \n",
    "          train_data, \n",
    "          seq_len=seq_len, \n",
    "          n_epochs=n_epochs, \n",
    "          device=device, \n",
    "          optimizer=optimizer, \n",
    "          lr=lr, \n",
    "          weight_decay=weight_decay, \n",
    "          loss_fn=loss_fn)\n",
    "\n",
    "evaluate_ptb(model,\n",
    "             train_data,\n",
    "             seq_len=seq_len,\n",
    "             device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs =100\n",
    "\n",
    "model = LanguageModel(embedding_size=embedding_size, output_size=len(corpus.dictionary), hidden_size=hidden_size, num_layers=num_layers, seq_model='lstm', device=device)\n",
    "train_ptb(model, train_data, seq_len=seq_len, n_epochs=n_epochs, device=device, optimizer=optimizer, lr=lr, weight_decay=weight_decay, loss_fn=loss_fn)\n",
    "evaluate_ptb(model, train_data, seq_len=seq_len, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs =100\n",
    "\n",
    "model = LanguageModel(embedding_size=embedding_size, output_size=len(corpus.dictionary), hidden_size=hidden_size, num_layers=num_layers, seq_model='gru', device=device)\n",
    "train_ptb(model, train_data, seq_len=seq_len, n_epochs=n_epochs, device=device, optimizer=optimizer, lr=lr, weight_decay=weight_decay, loss_fn=loss_fn)\n",
    "evaluate_ptb(model, train_data, seq_len=seq_len, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
