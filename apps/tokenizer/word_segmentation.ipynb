{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词应用: 基于一阶概率语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/playground/sgd_deep_learning/sgd_nlp\n"
     ]
    }
   ],
   "source": [
    "# 环境配置\n",
    "%cd /playground/sgd_deep_learning/sgd_nlp/\n",
    "import sys \n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgd_nlp.tokenizer import MaxProbabilityPathTokenizer\n",
    "from sgd_nlp.common import yield_tokens_list\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(tokenizer, training_tokens_list):\n",
    "    # step3: fit training data in model\n",
    "    tokenizer.clear()\n",
    "    tokenizer.train(training_tokens_list)\n",
    "\n",
    "    # todo 保存统计数据，不要每次都从新训练\n",
    "    # tokenizers.save_state()\n",
    "    # tokenizers.load_state('')\n",
    "    pass\n",
    "\n",
    "\n",
    "def model_inference(tokenizer, test_data):\n",
    "    for test_string in test_data:\n",
    "        print(tokenizer.inference(test_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # training_file path\n",
    "    data_home = r\"./data\" \\\n",
    "                r\"/tokenizer/backoff2005/training\"\n",
    "    train_file = \"msr_training.utf8\"\n",
    "    training_file_path = os.path.join(data_home, train_file)\n",
    "\n",
    "    # prepare training data in tokens form\n",
    "    training_tokens_list = yield_tokens_list(training_file_path, encoding='utf-8')\n",
    "\n",
    "    return training_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['唯一', '打破', '这', '一', '岑寂', '的', '是', '这里', '的', '山泉', '。'], -239.46884967138084)\n",
      "(['工', '信', '处女', '干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '2', '4', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作'], -442.09633785485664)\n",
      "(['多年来', '，', '中', '希', '贸易', '始终', '处于', '较低', '的', '水平', '，', '希腊', '几乎', '没有', '在', '中国', '投资', '。'], -368.41361487904726)\n"
     ]
    }
   ],
   "source": [
    "# step1 load data\n",
    "training_tokens_list = load_data()\n",
    "\n",
    "# step2 new model\n",
    "tokenizer = MaxProbabilityPathTokenizer()\n",
    "\n",
    "# step3 model train\n",
    "model_training(tokenizer, training_tokens_list)\n",
    "\n",
    "# step4 model inference\n",
    "test_data = [r'唯一打破这一岑寂的是这里的山泉。',\n",
    "                r'工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作',\n",
    "                r'多年来，中希贸易始终处于较低的水平，希腊几乎没有在中国投资。']\n",
    "model_inference(tokenizer, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
